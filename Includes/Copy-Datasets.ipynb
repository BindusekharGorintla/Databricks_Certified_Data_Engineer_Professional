{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78bf9669-c8fb-4388-8352-f9e4e6e09c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "195dbc57-f778-4156-b346-29ac372d25cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def path_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    msg = str(e)\n",
    "    if (\"com.databricks.sql.io.CloudFileNotFoundException\" in msg\n",
    "        or \"java.io.FileNotFoundException\" in msg):\n",
    "      return False\n",
    "    else:\n",
    "      raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "805b933b-6db7-449c-b795-681862b941aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CourseDataset:\n",
    "    def __init__(self, uri, data_catalog, db_name, location=None, checkpoint=None):\n",
    "        self.uri = uri\n",
    "        self.dataset_path = location\n",
    "        self.checkpoint_path = checkpoint\n",
    "        self.catalog_name = data_catalog\n",
    "        self.db_name = db_name\n",
    "    \n",
    "    def download_dataset(self):\n",
    "        source = self.uri\n",
    "        target = self.dataset_path\n",
    "\n",
    "        if self.catalog_name == \"hive_metastore\":\n",
    "            try:\n",
    "                spark.conf.set(\"fs.s3a.endpoint\", \"s3.eu-west-3.amazonaws.com\")\n",
    "                spark.conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        files = dbutils.fs.ls(source)\n",
    "\n",
    "        for f in files:\n",
    "            source_path = f\"{source}/{f.name}\"\n",
    "            target_path = f\"{target}/{f.name}\"\n",
    "            if not path_exists(target_path):\n",
    "                print(f\"Copying {f.name} ...\")\n",
    "                dbutils.fs.cp(source_path, target_path, True)\n",
    "    \n",
    "    \n",
    "    def create_database(self):\n",
    "        spark.sql(f\"USE CATALOG {self.catalog_name}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.db_name}\")\n",
    "        spark.sql(f\"USE SCHEMA {self.db_name}\")\n",
    "\n",
    "        print(f\"Data catalog: {self.catalog_name}\")\n",
    "        print(f\"Schema: {self.db_name}\")\n",
    "\n",
    "        self.__configure_directories()\n",
    "    \n",
    "    \n",
    "    def clean_up(self):\n",
    "        if self.catalog_name == \"hive_metastore\":\n",
    "            print(\"Removing Checkpoints ...\")\n",
    "            dbutils.fs.rm(self.checkpoint_path, True)\n",
    "            print(\"Dropping Database ...\")\n",
    "            spark.sql(f\"DROP SCHEMA IF EXISTS {self.db_name} CASCADE\")\n",
    "            print(\"Removing Dataset ...\")\n",
    "            dbutils.fs.rm(self.dataset_path, True)\n",
    "        else:\n",
    "            print(\"Dropping Database, Dataset, and Checkpoints ...\")\n",
    "            spark.sql(f\"DROP SCHEMA IF EXISTS {self.db_name} CASCADE\")\n",
    "        print(\"Done\")\n",
    "\n",
    "\n",
    "    def __configure_directories(self):\n",
    "        dataset_volume_name = \"dataset\"\n",
    "        checkpoints_volume_name = \"checkpoints\"\n",
    "\n",
    "        if self.catalog_name == \"hive_metastore\":\n",
    "            self.dataset_path = 'dbfs:/mnt/demo-datasets/DE-Pro/bookstore'\n",
    "            self.checkpoint_path = \"dbfs:/mnt/demo_pro/checkpoints\"\n",
    "        else:\n",
    "            volume_root = f\"/Volumes/{self.catalog_name}/{self.db_name}\"\n",
    "            self.dataset_path = f\"{volume_root}/{dataset_volume_name}\"\n",
    "            self.checkpoint_path = f\"{volume_root}/{checkpoints_volume_name}\"\n",
    "            \n",
    "            spark.sql(f\"CREATE VOLUME IF NOT EXISTS {dataset_volume_name}\")\n",
    "            spark.sql(f\"CREATE VOLUME IF NOT EXISTS {checkpoints_volume_name}\")\n",
    "    \n",
    "    def __get_index(self, dir):\n",
    "        try:\n",
    "            files = dbutils.fs.ls(dir)\n",
    "            file = max(f.name for f in files if f.name.endswith('.json'))\n",
    "            index = int(file.rsplit('.', maxsplit=1)[0])\n",
    "        except:\n",
    "            index = 0\n",
    "        return index+1\n",
    "    \n",
    "    \n",
    "    def __load_json_file(self, current_index, streaming_dir, raw_dir):\n",
    "        latest_file = f\"{str(current_index).zfill(2)}.json\"\n",
    "        source = f\"{streaming_dir}/{latest_file}\"\n",
    "        target = f\"{raw_dir}/{latest_file}\"\n",
    "        prefix = streaming_dir.split(\"/\")[-1]\n",
    "        if path_exists(source):\n",
    "            print(f\"Loading {prefix}-{latest_file} file to the bookstore dataset\")\n",
    "            dbutils.fs.cp(source, target)\n",
    "    \n",
    "    \n",
    "    def __load_data(self, max, streaming_dir, raw_dir, all=False):\n",
    "        index = self.__get_index(raw_dir)\n",
    "        if index > max:\n",
    "            print(\"No more data to load\\n\")\n",
    "            return 0\n",
    "\n",
    "        elif all == True:\n",
    "            while index <= max:\n",
    "                self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "                index += 1\n",
    "        else:\n",
    "            self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "            index += 1\n",
    "\n",
    "        return 1\n",
    "    \n",
    "    def load_new_data(self, num_files = 1):\n",
    "        streaming_dir = f\"{self.dataset_path}/kafka-streaming\"\n",
    "        raw_dir = f\"{self.dataset_path}/kafka-raw\"\n",
    "        for i in range(num_files):\n",
    "            self.__load_data(10, streaming_dir, raw_dir)\n",
    "\n",
    "    def load_books_updates(self):\n",
    "        streaming_dir = f\"{self.dataset_path}/books-updates-streaming\"\n",
    "        raw_dir = f\"{self.dataset_path}/kafka-raw/books-updates\"\n",
    "        self.__load_data(5, streaming_dir, raw_dir)\n",
    "\n",
    "    def load_pipeline_data(self):\n",
    "        streaming_dir = f\"{self.dataset_path}/kafka-streaming\"\n",
    "        raw_dir = f\"{self.dataset_path}/kafka-raw-etl\"\n",
    "        n = self.__load_data(10, streaming_dir, raw_dir)\n",
    "\n",
    "        books_streaming_dir = f\"{self.dataset_path}/books-updates-streaming\"\n",
    "        books_raw_dir = f\"{self.dataset_path}/kafka-raw-etl/books-updates\"\n",
    "        m = self.__load_data(5, books_streaming_dir, books_raw_dir)\n",
    "\n",
    "        return n + m\n",
    " \n",
    "    def process_bronze(self):\n",
    "        schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                            .format(\"cloudFiles\")\n",
    "                            .option(\"cloudFiles.format\", \"json\")\n",
    "                            .schema(schema)\n",
    "                            .load(f\"{self.dataset_path}/kafka-raw\")\n",
    "                            .withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))  \n",
    "                            .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "                      .writeStream\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint_path}/bronze\")\n",
    "                          .option(\"mergeSchema\", True)\n",
    "                          .partitionBy(\"topic\", \"year_month\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .table(\"bronze\"))\n",
    "\n",
    "        query.awaitTermination()\n",
    "    \n",
    "    @staticmethod\n",
    "    def upsert_orders_batch(microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "    \n",
    "        sql_query = \"\"\"\n",
    "          MERGE INTO orders_silver a\n",
    "          USING orders_microbatch b\n",
    "          ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp\n",
    "          WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "        \n",
    "    @staticmethod\n",
    "    def upsert_customers_batch(microBatchDF, batchId):\n",
    "        window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "        \n",
    "        (microBatchDF.filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "                     .withColumn(\"rank\", F.rank().over(window))\n",
    "                     .filter(\"rank == 1\")\n",
    "                     .drop(\"rank\")\n",
    "                     .createOrReplaceTempView(\"ranked_updates\"))\n",
    "\n",
    "        query = \"\"\"\n",
    "            MERGE INTO customers_silver c\n",
    "            USING ranked_updates r\n",
    "            ON c.customer_id=r.customer_id\n",
    "                WHEN MATCHED AND c.row_time < r.row_time\n",
    "                  THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED\n",
    "                  THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(query)\n",
    "        \n",
    "    @staticmethod\n",
    "    def upsert_books_batch(microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "        sql_query = \"\"\"\n",
    "            MERGE INTO books_silver\n",
    "            USING (\n",
    "                SELECT updates.book_id as merge_key, updates.*\n",
    "                FROM updates\n",
    "\n",
    "                UNION ALL\n",
    "\n",
    "                SELECT NULL as merge_key, updates.*\n",
    "                FROM updates\n",
    "                JOIN books_silver ON updates.book_id = books_silver.book_id\n",
    "                WHERE books_silver.current = true AND updates.price <> books_silver.price\n",
    "              ) staged_updates\n",
    "            ON books_silver.book_id = merge_key \n",
    "            WHEN MATCHED AND books_silver.current = true AND books_silver.price <> staged_updates.price THEN\n",
    "              UPDATE SET current = false, end_date = staged_updates.updated\n",
    "            WHEN NOT MATCHED THEN\n",
    "              INSERT (book_id, title, author, price, current, effective_date, end_date)\n",
    "              VALUES (staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "    \n",
    "    def process_orders_silver(self):\n",
    "        json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "        \n",
    "        deduped_df = (spark.readStream\n",
    "                   .table(\"bronze\")\n",
    "                   .filter(\"topic = 'orders'\")\n",
    "                   .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                   .select(\"v.*\")\n",
    "                   .withWatermark(\"order_timestamp\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"order_id\", \"order_timestamp\"]))\n",
    "        \n",
    "        query = (deduped_df.writeStream\n",
    "                   .foreachBatch(CourseDataset.upsert_orders_batch)\n",
    "                   .outputMode(\"update\")\n",
    "                   .option(\"checkpointLocation\", f\"{self.checkpoint_path}/orders_silver\")\n",
    "                   .trigger(availableNow=True)\n",
    "                   .start())\n",
    "\n",
    "        query.awaitTermination()\n",
    "\n",
    "        \n",
    "    def process_customers_silver(self):\n",
    "        \n",
    "        schema = \"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp\"\n",
    "        \n",
    "        df_country_lookup = spark.read.json(f\"{self.dataset_path}/country_lookup\")\n",
    "\n",
    "        query = (spark.readStream\n",
    "                          .table(\"bronze\")\n",
    "                          .filter(\"topic = 'customers'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                          .select(\"v.*\")\n",
    "                          .join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\") , \"inner\")\n",
    "                       .writeStream\n",
    "                          .foreachBatch(CourseDataset.upsert_customers_batch)\n",
    "                          .outputMode(\"update\")\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint_path}/customers_silver\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "    \n",
    "    def process_books_silver(self):\n",
    "        schema = \"book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                        .table(\"bronze\")\n",
    "                        .filter(\"topic = 'books'\")\n",
    "                        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                        .select(\"v.*\")\n",
    "                     .writeStream\n",
    "                        .foreachBatch(CourseDataset.upsert_books_batch)\n",
    "                        .option(\"checkpointLocation\", f\"{self.checkpoint_path}/books_silver\")\n",
    "                        .trigger(availableNow=True)\n",
    "                        .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "        \n",
    "    def process_current_books(self):\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TABLE current_books\n",
    "            AS SELECT book_id, title, author, price\n",
    "               FROM books_silver\n",
    "               WHERE current IS TRUE\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca15a1c-26af-4019-a97a-c29b5d077a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_source_uri = \"s3://dalhussein-courses/DE-Pro/datasets/bookstore/v1/\"\n",
    "db_name = \"bookstore_eng_pro\"\n",
    "\n",
    "data_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "bookstore = CourseDataset(data_source_uri, data_catalog, db_name)\n",
    "\n",
    "bookstore.create_database()\n",
    "bookstore.download_dataset()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Copy-Datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}